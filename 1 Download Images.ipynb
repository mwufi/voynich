{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the data\n",
    "\n",
    "This notebook is for building the data that the rest of them will use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can get the images from voynichese.com site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vdata import Voynich\n",
    "\n",
    "def downloadImages():\n",
    "    a = Voynich()\n",
    "    import urllib\n",
    "\n",
    "    save_dir = 'image/'\n",
    "    base_url = 'http://www.voynichese.com/2/data/folio/image/glance/color/large/'\n",
    "\n",
    "    for folio in a.folios:\n",
    "        img_name = '{}.jpg'.format(folio)\n",
    "        url = base_url + img_name\n",
    "        urllib.request.urlretrieve(url, save_dir + img_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "So we get this data from voynichese.com. One file for each page, and they look like this!\n",
    "\n",
    "f100r.xml\n",
    "\n",
    "```\n",
    "<folio name=\"f100r\" wordCount=\"111\" width=\"1083\" height=\"1500\">\n",
    "  <word index=\"0\" x=\"159\" y=\"84\" width=\"105\" height=\"31\">chosaroshol</word>\n",
    "  <word index=\"1\" x=\"280\" y=\"69\" width=\"93\" height=\"44\">sochorcfhy</word>\n",
    "  ...\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "DIR = 'data/'\n",
    "\n",
    "# --------------- for working with the data/takeshi.txt file ----------------\n",
    "def process(line):\n",
    "    line = line.strip()\n",
    "    line = re.sub('-','.',line)\n",
    "    line = re.sub('[\\!<>\\*\\=\\%]', '', line)\n",
    "    header, line = line.split(' ', maxsplit=1)\n",
    "    header = re.sub('[;\\.]', ' ', header).split()\n",
    "    line = line.replace('.',' ')\n",
    "    return header, line.strip()\n",
    "\n",
    "def pdFromFile(filename):\n",
    "    with open(DIR + filename) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        df = []\n",
    "        for line in lines:\n",
    "            header, text = process(line)\n",
    "            df.append({\n",
    "                'folio': header[0],\n",
    "                'paragraph': header[1],\n",
    "                'line': header[2],\n",
    "                'text': text,\n",
    "            })\n",
    "        \n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "    \n",
    "def VoynichOrdering(folio_name):\n",
    "    \"\"\"\n",
    "    only used as an argument to sort()\n",
    "    \"\"\"\n",
    "    page, part = re.match('f(\\d+)(.+)', folio_name).groups()\n",
    "    page = int(page)\n",
    "    return page, part\n",
    "\n",
    "\n",
    "class Voynich:\n",
    "    \"\"\"\n",
    "    contains a dataFrame of all the folios (w/ line and paragraph numbers)\n",
    "    with utility methods to get words and lines from a single folio\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = pdFromFile('takeshi.txt')\n",
    "        self.folios = sorted(set(self.data.folio), key=VoynichOrdering)\n",
    "        \n",
    "    def getWords(self, number):\n",
    "        return ' '.join(self.getLines(number))\n",
    "    \n",
    "    def getLines(self, number):\n",
    "        lines = list(self.data[self.data.folio == number].text)\n",
    "        return lines\n",
    "    \n",
    "\n",
    "# ------------------------ for working with data/*.xml files -------------------\n",
    "import os\n",
    "from pathlib import Path\n",
    "class Folio:\n",
    "    \n",
    "    voynich = Voynich()\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "        self.data = None\n",
    "        \n",
    "        self.filename = Path(os.path.join(DIR, name + '.xml'))\n",
    "        if self.filename.exists():\n",
    "            self.readData()\n",
    "        else:\n",
    "            print('no file exist')\n",
    "    \n",
    "    def readData(self):\n",
    "         with open(self.filename,\"r\") as infile:\n",
    "            contents = infile.read()\n",
    "\n",
    "            # build pandas dict\n",
    "            soup = BeautifulSoup(contents,'xml')\n",
    "            titles = soup.find_all('word')\n",
    "\n",
    "            def parse(t):\n",
    "                a = dict()\n",
    "                for prop in ['x', 'y', 'width', 'height']:\n",
    "                    a[prop] = int(t[prop])\n",
    "                a['word'] = t.get_text()\n",
    "                return a\n",
    "\n",
    "            data = [parse(t) for t in titles]\n",
    "            self.data = pd.DataFrame(data)\n",
    "            \n",
    "            self.wordcount = int(soup.find('folio')['wordCount'])\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def lines(self):\n",
    "        return Folio.voynich.getLines(self.name)\n",
    "            \n",
    "    @property\n",
    "    def text(self):\n",
    "        if self.data is not None:\n",
    "            return ' '.join(self.data.word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fachys ykal ar ataiin shol shory cthres y kor sholdy',\n",
       " 'sory ckhar or y kair chtaiin shar are cthar cthar dan',\n",
       " 'syaiir sheky or ykaiin shod cthoary cthes daraiin sa',\n",
       " 'ooiin oteey oteos roloty cthar daiin otaiin or okan',\n",
       " 'dair y chear cthaiin cphar cfhaiin',\n",
       " 'ydaraishy',\n",
       " 'odar o y shol cphoy oydar sh s cfhoaiin shodary',\n",
       " 'yshey shody okchoy otchol chocthy oschy dain chor kos',\n",
       " 'daiin shos cfhol shody',\n",
       " 'dain os teody',\n",
       " 'ydain cphesaiin ol s cphey ytain shoshy cphodales',\n",
       " 'oksho kshoy otairin oteol okan shodain sckhey daiin',\n",
       " 'shoy ckhey kodaiin cphy cphodaiils cthey she oldain d',\n",
       " 'dain oiin chol odaiin chodain chdy okain dan cthy kod',\n",
       " 'daiin shckhey ckeor chor shey kol chol chol kor chal',\n",
       " 'sho chol shodan kshy kchy dor chodaiin sho kchom',\n",
       " 'ycho tchey chokain sheo pshol dydyd cthy daicthy',\n",
       " 'yto shol she kodshey cphealy dasain dain ckhyds',\n",
       " 'dchar shcthaiin okaiir chey rchy potol cthols dlocta',\n",
       " 'shok chor chey dain ckhey',\n",
       " 'otol daiiin',\n",
       " 'cpho shaiin shokcheey chol tshodeesy shey pydeey chy ro d',\n",
       " 'doin chol dain cthal dar shear kaiin dar shey cthar',\n",
       " 'choo kaiin shoaiin okol daiin far cthol daiin ctholdar',\n",
       " 'ycheey okay oky daiin okchey kokaiin chol kchy dal',\n",
       " 'deeo shody koshey cthy okchey keey keey dal chtor',\n",
       " 'eo chol chok choty chotey',\n",
       " 'dchaiin']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Folio('f1r').lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fachys ykal ar ataiin shol shory cthres y kor sholdy sory ckhar or y kair chtaiin shar are cthar cthar dan syaiir sheky or ykaiin shod cthoary cthes daraiin sa ooiin oteey oteos roloty cthar daiin otaiin or okan dair y chear cthaiin cphar cfhaiin ydaraishy odar o y shol cphoy oydar sh s cfhoaiin shodary yshey shody okchoy otchol chocthy oschy dain chor kos daiin shos cfhol shody dain os teody ydain cphesaiin ol s cphey ytain shoshy cphodales oksho kshoy otairin oteol okan shodain sckhey daiin shoy ckhey kodaiin cphy cphodaiils cthey she oldain d dain oiin chol odaiin chodain chdy okain dan cthy kod daiin shckhey ckeor chor shey kol chol chol kor chal sho chol shodan kshy kchy dor chodaiin sho kchom ycho tchey chokain sheo pshol dydyd cthy daicthy yto shol she kodshey cphealy dasain dain ckhyds dchar shcthaiin okaiir chey rchy potol cthols dlocta shok chor chey dain ckhey otol daiiin cpho shaiin shokcheey chol tshodeesy shey pydeey chy ro d doin chol dain cthal dar shear kaiin dar shey cthar choo kaiin shoaiin okol daiin far cthol daiin ctholdar ycheey okay oky daiin okchey kokaiin chol kchy dal deeo shody koshey cthy okchey keey keey dal chtor eo chol chok choty chotey dchaiin'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Folio('f1r').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIntegrity():\n",
    "    a = Voynich()\n",
    "    for f in a.folios:\n",
    "        arr1, arr2 = a.getWords(f).split(), Folio(f).text.split()\n",
    "        assert arr1 == arr2\n",
    "\n",
    "# checkIntegrity()\n",
    "# yes! all the folios match up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do things like select by paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f100r', 'L1') 7 words ['chosaroshol', 'sochorcfhy', 'otear', 'chofary', 'sar']\n",
      "('f100r', 'L2') 6 words ['osaro', 'chalsain', 'soity', 'sosam', 'dakocth']\n",
      "('f100r', 'L3') 6 words ['okeeos', 'shockhey', 'orol', 'olcheom', 'okols']\n",
      "('f100r', 'P1') 36 words ['pcheol', 'sheod', 'qocpheeckhy', 'shodol', 'cthdaoto']\n",
      "('f100r', 'P2') 56 words ['folshody', 'chol', 'daiin', 'fchodycheol', 'cphol']\n",
      "('f100v', 'B') 5 words ['okcheor', 'ytchol', 'dykchal', 'chos', 'cthoral']\n",
      "('f100v', 'M') 6 words ['sol', 'eesos', 'ykchochdy', 'ykchdy', 'dchdy']\n",
      "('f100v', 'P') 76 words ['cthdeecthy', 'sheocphy', 'qoteody', 'ckhoor', 'ar']\n",
      "('f100v', 'T') 4 words ['tolchd', 'chols', 'opchor', 'rolsy']\n",
      "('f101r1', 'P') 209 words ['pcheol', 'cheol', 'ol', 'shey', 'qockhol']\n",
      "('f101v2', 'P') 185 words ['tolchor', 'cheopor', 'or', 'ody', 'cpheyr']\n",
      "('f101v2', 'R1') 9 words ['sairaly', 'otaldy', 'otal', 'yta', 'dokor']\n",
      "('f101v2', 'R2') 10 words ['okol', 'arom', 'orar', 'am', 'or']\n",
      "('f101v2', 'T') 9 words ['teol', 'cheol', 'ekchey', 'or', 'cheol']\n",
      "('f102r1', 'L1') 4 words ['dordod', 'orolaly', 'dardsh', 'otodeeodor']\n",
      "('f102r1', 'P') 105 words ['polaiin', 'shocthy', 'qoteol', 'loiiin', 'oteeor']\n",
      "('f102r2', 'P') 129 words ['lsais', 'amg', 'cheey', 'cfhey', 'por']\n",
      "('f102r2', 'X') 5 words ['dedloy', 'osal', 'ralr', 'koldarod', 'odalydary']\n",
      "('f102v1', 'L1') 7 words ['ker', 'ron', 'ykeody', 'okeody', 'doiisaly']\n",
      "('f102v1', 'L2') 3 words ['ypcholdy', 'loralody', 'opchdard']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "a = Voynich()\n",
    "# join by paragraph\n",
    "paragraphs = a.data.groupby(['folio', 'paragraph']).agg({\n",
    "    'text': '\\n'.join\n",
    "}).to_dict()['text']\n",
    "\n",
    "for k,v in itertools.islice(paragraphs.items(), 20):\n",
    "    print(k, len(v.split()), 'words', v.split()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BELOW THIS IS NOT REALLY USEFUL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating phrases in the manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hi', 'i'), ('i', 'saw'), ('saw', 'a'), ('a', 'dog')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'hi i saw a dog'\n",
    "bigrm = list(nltk.bigrams(text.split()))\n",
    "bigrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "defaultdict(default_factory[, ...]) --> dict with default factory\n",
       "\n",
       "The default factory is called without arguments to produce\n",
       "a new value when a key is not present, in __getitem__ only.\n",
       "A defaultdict compares equal to a dict with the same items.\n",
       "All remaining arguments are treated the same as if they were\n",
       "passed to the dict constructor, including keyword arguments.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.6/collections/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "?defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = defaultdict(list)\n",
    "counts = Counter()\n",
    "def add(bigrams):\n",
    "    counts.update([x[0] for x in bigrams])\n",
    "    for x,folio,line in bigrams:\n",
    "        locations[x] += [(folio, line)]\n",
    "        \n",
    "for index, row in takeshi.iterrows():\n",
    "    folio, line, paragraph, text = row['folio'], row['line'], row['paragraph'], row['text']\n",
    "    bigrm = list(nltk.bigrams(text.split()))\n",
    "    bigrm = [(x, folio, line) for x in bigrm]\n",
    "    add(bigrm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(locations[('or', 'aiin')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('chedy', 'shedy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8493\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(voynich_text)\n",
    "unique_words = list(set(tokens))\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cphedy', 'kchedy', 'ochedy', 'chydy', 'chesy']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def searchForClose(word):\n",
    "    minpairs = [w for w in unique_words if edit_distance(w, word) == 1]\n",
    "    return minpairs\n",
    "\n",
    "searchForClose('chedy')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-b20615e6dfd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearchForClose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-c42d02fbe616>\u001b[0m in \u001b[0;36msearchForClose\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msearchForClose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mminpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_words\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0medit_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mminpairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msearchForClose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chedy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-c42d02fbe616>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msearchForClose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mminpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_words\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0medit_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mminpairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msearchForClose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chedy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/metrics/distance.py\u001b[0m in \u001b[0;36medit_distance\u001b[0;34m(s1, s2, substitution_cost, transpositions)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             _edit_dist_step(lev, i + 1, j + 1, s1, s2,\n\u001b[0m\u001b[1;32m     91\u001b[0m                             substitution_cost=substitution_cost, transpositions=transpositions)\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(unique_words)\n",
    "\n",
    "for w1 in unique_words:\n",
    "    for w2 in searchForClose(w1):\n",
    "        G.add_edge(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz as gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('or', 'aiin'), 51),\n",
       " (('chol', 'daiin'), 33),\n",
       " (('s', 'aiin'), 28),\n",
       " (('ar', 'al'), 23),\n",
       " (('shedy', 'qokedy'), 21),\n",
       " (('ar', 'aiin'), 21),\n",
       " (('ol', 'chedy'), 21),\n",
       " (('ol', 'shedy'), 21),\n",
       " (('chol', 'chol'), 19),\n",
       " (('qol', 'chedy'), 19),\n",
       " (('chedy', 'qokeey'), 19),\n",
       " (('shedy', 'qokaiin'), 18),\n",
       " (('chedy', 'qokain'), 18),\n",
       " (('qokeedy', 'qokeedy'), 18),\n",
       " (('daiin', 'daiin'), 17),\n",
       " (('shey', 'qokain'), 17),\n",
       " (('shedy', 'qokeedy'), 16),\n",
       " (('chedy', 'qokedy'), 14),\n",
       " (('chedy', 'qol'), 14),\n",
       " (('daiin', 'chey'), 13)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are a superset of the labels\n",
    "\n",
    "https://github.com/voynichese/voynichese/blob/wiki/Experiments.md#label-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeshi[takeshi.paragraph.str.contains('T')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teol cheol ekcheyor cheol cthol cholaiin chol qkos'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "takeshi.iloc[4030].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_ed12b60d4e79f489850662e92c114122 NOW.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pystan\n",
    "\n",
    "# bernoulli model\n",
    "model_code = \"\"\"\n",
    "    data {\n",
    "      int<lower=0> N;\n",
    "      int<lower=0,upper=1> y[N];\n",
    "    }\n",
    "    parameters {\n",
    "      real<lower=0,upper=1> theta;\n",
    "    }\n",
    "    model {\n",
    "      for (n in 1:N)\n",
    "          y[n] ~ bernoulli(theta);\n",
    "    }\n",
    "    \"\"\"\n",
    "data = dict(N=10, y=[0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "model = pystan.StanModel(model_code=model_code)\n",
    "fit = model.sampling(data=data)\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load it at some future point\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# run with different data\n",
    "fit = model.sampling(data=dict(N=5, y=[1, 1, 0, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inference for Stan model: anon_model_ed12b60d4e79f489850662e92c114122.\n",
       "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
       "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
       "\n",
       "        mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
       "theta   0.58  5.0e-3   0.17   0.24   0.46   0.59    0.7   0.87   1166    1.0\n",
       "lp__   -5.28    0.02   0.69  -7.26  -5.45   -5.0  -4.83  -4.78   1706    1.0\n",
       "\n",
       "Samples were drawn using NUTS at Sat Apr 21 23:39:32 2018.\n",
       "For each parameter, n_eff is a crude measure of effective sample size,\n",
       "and Rhat is the potential scale reduction factor on split chains (at \n",
       "convergence, Rhat=1)."
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.linspace(-3, 3, num=50)\n",
    "y_train = np.cos(x_train) + np.random.normal(0, 0.1, size=50)\n",
    "x_train = x_train.astype(np.float32).reshape((50, 1))\n",
    "y_train = y_train.astype(np.float32).reshape((50, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from edward.models import Normal\n",
    "\n",
    "W_0 = Normal(loc=tf.zeros([1, 2]), scale=tf.ones([1, 2]))\n",
    "W_1 = Normal(loc=tf.zeros([2, 1]), scale=tf.ones([2, 1]))\n",
    "b_0 = Normal(loc=tf.zeros(2), scale=tf.ones(2))\n",
    "b_1 = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "\n",
    "x = x_train\n",
    "y = Normal(loc=tf.matmul(tf.tanh(tf.matmul(x, W_0) + b_0), W_1) + b_1,\n",
    "           scale=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f41404c3438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helpful methods\n",
    "\n",
    "def showLocations(a):\n",
    "    \"\"\"\n",
    "    params\n",
    "        a: Folio\n",
    "    returns: picture of the word positions\n",
    "    \"\"\"\n",
    "    x,y = a.data.x, a.data.y\n",
    "    plt.plot(x,-y, 'r+')\n",
    "    plt.title(a.name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chosaroshol sochorcfhy otear chofary sar char daiindy osaro chalsain soity sosam dakocth sofal pcheol sheod qocpheeckhy shodol cthdaoto ch qeos sheey chcthso s dsheor cthey qokeey oteey ykeeodain sorary daiin daiin deeamshol shor chkeey qoteey qokeody qoteold qokeol so raiin otal ykecho dcheor shol qokeeol chor chol qokeeody dorean okeeos shockhey orol olcheom okols oteol folshody chol daiin fchodycheol cphol qotees shey oreso alcfhy soiin chol cphol shol shol qockhol chor chol sho keey cckhhy ykeeam saiichor sheor qockhody odeor yksheey chol sheody sai cheol raiin sheor qkeeody chol daiin ctheol olcheol cheky cheol cheockhy okeol yaiin chekeey chol cholody chos olchor qokeol okeeol cheols al chol cheol cho chckheody otolchey'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
